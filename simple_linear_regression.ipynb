{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from random import gauss\n",
    "from lin_reg import best_line\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- explain and use the concepts of covariance and correlation;\n",
    "- explain how to interpret linear regressions;\n",
    "- describe the assumptions of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation\n",
    "\n",
    "The idea of _correlation_ is the simple idea that variables often change _together_. For a simple example, cities with more buses tend to have higher populations.\n",
    "\n",
    "We might observe that, as one variable X increases, so does another Y, OR that as X increases, Y decreases.\n",
    "\n",
    "The _covariance_ describes how two variables co-vary. Note the similarity in the definition to the definition of ordinary variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "For two random variables $X$ and $Y$, each with $n$ values:\n",
    "\n",
    "$\\Large\\sigma_{XY} = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{n}$ <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 3, 5]\n",
    "Y = [2, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance by hand:\n",
    "((1-3) * (2-7) + (3-3) * (9-7) + (5-3) * (10-7)) / 3\n",
    "\n",
    "# Better yet: With NumPy:\n",
    "np.cov(X, Y, ddof=0)[0, 1]\n",
    "\n",
    "np.cov(X, Y, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the value of the covariance is very much a function of the values of X and Y, which can make interpretation difficult. What is wanted is a _standardized_ scale for covariance, hence: _correlation_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Pearson Correlation:<br/>$\\Large r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\mu_x)^2\\Sigma^n_{i = 1}(y_i -\\mu_y)^2}}$\n",
    "\n",
    "Note that we are simply standardizing the covariance by the standard deviations of X and Y (the $n$'s cancel!).\n",
    "\n",
    "$\\bf{Check}$:\n",
    "\n",
    "<details><summary>\n",
    "What happens if X = Y?\n",
    "</summary>\n",
    "Then numerator = denominator and the correlation = 1!\n",
    "</details>\n",
    "<br/>\n",
    "We'll always have $-1 \\leq r \\leq 1$. (This was the point of standardizing by the standard deviations of X and Y.)\n",
    "\n",
    "A correlation of -1 means that X and Y are perfectly negatively correlated, and a correlation of 1 means that X and Y are perfectly positively correlated.\n",
    "\n",
    "NumPy also has a correlation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 / np.sqrt(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X, Y)[0, 1] == (np.cov(X, Y, ddof=0) / (np.std(X) * np.std(Y)))[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so does SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(X, Y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causation\n",
    "\n",
    "_Why_ does it happen that variables correlate? It _may_ be that one is the cause of the other. A city having a high population, for example, probably does have some causal effect on the number of buses that the city has. But this _need not_ be the case, and that is why statisticians are fond of saying that 'correlation is not causation'. An alternative possibility, for example, is that high values of X and Y are _both_ caused by high values of some third factor Z. The size of children's feet, for example, is correlated with their ability to spell, but this is of course NOT because either is a cause of the other. Rather, BOTH are caused by the natural maturing and development of children. As they get older, both their feet and their spelling abilities grow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning Theory\n",
    "\n",
    "It's important at this point to understand the distinction between dependent and independent variables.\n",
    "\n",
    "Roughly, the independent variable is what can be directly manipulated and the dependent variable is what cannot be (but is nevertheless of great interest). What matters structurally is simply that we understand the dependent variable to be a _function_ of the independent variable(s).\n",
    "\n",
    "This is the proper interpretation of a statistical _model_.\n",
    "\n",
    "Simple idea: We can model correlation with a _line_. As one variable changes, so does the other.\n",
    "\n",
    "This model has two *parameters*: *slope* and *y-intercept*.\n",
    "\n",
    "Unless there's a perfectly (and suspiciously) linear relationship between our predictor(s) and our target, there will  be some sort of **error** or **loss** or **residual**. The best-fit line is constructed by minimizing the sum of the squares of these losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Equation\n",
    "\n",
    "The solution for a simple regression best-fit line is as follows:\n",
    "\n",
    "- slope: <br/>$\\Large m = r_P\\frac{\\sigma_y}{\\sigma_x} = \\frac{cov(X, Y)}{var(X)}$\n",
    "\n",
    "- y-intercept:<br/> $\\Large b = \\mu_y - m\\mu_x$\n",
    "\n",
    "### Proof\n",
    "\n",
    "We demonstrate this by setting the derivative of the loss function, $\\Sigma^n_{i=1}(y_i - (mx_i + b))^2$, equal to 0. **We shall see this calculus trick many times!**\n",
    "\n",
    "For this purpose we consider the loss a function of its optimizing parameters $m$ and $b$. So there are therefore two partial derivatives to consider. (We'll cover this in more depth later in the course.)\n",
    "\n",
    "(i) $\\frac{\\partial}{\\partial b}[\\sum^n_{i=1}(y_i - mx_i - b)^2] = -2\\sum^n_{i=1}(y_i - mx_i - b)$\n",
    "\n",
    "(ii) $\\frac{\\partial}{\\partial m}[\\sum^n_{i=1}(y_i - mx_i - b)^2] = -2\\sum^n_{i=1}x_i\\sum^n_{i=1}(y_i - mx_i - b)$\n",
    "\n",
    "- Let's set the first to 0:\n",
    "\n",
    "$-2\\sum^n_{i=1}(y_i - mx_i - b) = 0$ <br/>\n",
    "$\\sum^n_{i=1}(y_i - mx_i) = \\sum^n_{i=1}b = nb$ <br/>\n",
    "\n",
    "**So:** $\\large b = \\frac{\\sum^n_{i=1}(y_i - mx_i)}{n} = \\mu_y - m\\mu_x$\n",
    "\n",
    "- Let's set the second to 0:\n",
    "\n",
    "$-2\\sum^n_{i=1}x_i\\sum^n_{i=1}(y_i - mx_i - b) = 0$ <br/>\n",
    "$\\sum^n_{i=1}(x_iy_i - mx^2_i - bx_i) = 0$ <br/>\n",
    "\n",
    "- Plugging in our previous result, we have:\n",
    "\n",
    "$\\sum^n_{i=1}x_iy_i - (\\frac{1}{n}\\sum^n_{i=1}y_i - \\frac{m}{n}\\sum^n_{i=1}x_i)\\sum^n_{i=1}x_i - m\\sum^n_{i=1}x^2_i = 0$ <br/>\n",
    "$\\sum^n_{i=1}x_iy_i - \\frac{1}{n}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i + \\frac{m}{n}(\\sum^n_{i=1}x_i)^2 - m\\sum^n_{i=1}x^2_i = 0$ <br/>\n",
    "\n",
    "**So:** $\\large m = \\frac{\\sum^n_{i=1}x_iy_i - \\frac{1}{n}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i}{\\sum^n_{i=1}x^2_i - \\frac{1}{n}(\\sum^n_{i=1}x_i)^2} = \\frac{n\\times(\\frac{1}{n}\\sum^n_{i=1}x_iy_i - \\frac{1}{n^2}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i)}{n\\times(\\frac{1}{n}\\sum^n_{i=1}x^2_i - \\mu^2_x)} = \\frac{cov(X, Y)}{var(X)}$\n",
    "\n",
    "For more on the proof see [here](https://math.stackexchange.com/questions/716826/derivation-of-simple-linear-regression-parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The output of the simple linear regression algorithm is a pair of parameters: the slope and the y-intercept of the best-fit line through the data.\n",
    "\n",
    "***I therefore have a (more or less crude) MODEL of the phenomenon in question:***\n",
    "\n",
    "Suppose I have a bunch of data about (i) how many cigarettes people smoked in their lifetimes and (ii) how many years those same people lived. If I set my independent variable (\"x\") to be the number of cigarettes smoked and my dependent variable (\"y\") to be the number of years lived, then ***for any deceased person at all I will have a way of estimating the number of years that person lived if I know the number of cigarettes that that person smoked***. This estimate is exactly what the best-fit line gives me.\n",
    "\n",
    "Suppose the parameters of the regression come out to be $\\beta_0 = 100$ years and $\\beta_1 = -1\\times 10^{-4}$ years / cigarette ([in reality](https://www.medicalnewstoday.com/releases/9703#1) these are probably both a bit high).\n",
    "\n",
    "Then we would be modeling the lifespan of human beings according to the number of cigarettes smoked:\n",
    "\n",
    "$Y = \\beta_1\\times n + \\beta_0$,\n",
    "\n",
    "where $Y$ = the number of years (estimated) and $n$ is the number of cigarettes smoked.\n",
    "\n",
    "- If someone smoked 0 cigarettes, then we would estimate that person's lifespan as:\n",
    "\n",
    "$-1\\times 10^{-4}\\times 0 + 100 = 100$ years.\n",
    "\n",
    "- If someone smoked a pack a day for 30 years, that's 20 * 365 * 30 = 219000 cigarettes (never mind about leap years!), so we would estimate that person's lifespan as:\n",
    "\n",
    "$-1\\times 10^{-4}\\times 219000 + 100 = 78.1$ years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression:\n",
    "\n",
    "**1. The relationship between target and predictor(s) is linear. (Of course!)**\n",
    "\n",
    "**How can I check for this?**\n",
    "- Build a scatterplot of y vs. various predictors.\n",
    "\n",
    "**What can I do if it looks like I'm violating this assumption?**\n",
    "- Consider log-scaling your data.\n",
    "- Consider a different type of model!\n",
    "\n",
    "**2. The errors are mutually independent. (That is, there is no correlation between any two errors.)**\n",
    "\n",
    "**How can I check for this?**\n",
    "- Build an error plot, i.e. a plot of errors for a particular predictor (vs. the values of that predictor).\n",
    "\n",
    "**What can I do if it looks like I'm violating this assumption?**\n",
    "- Consider dropping extreme values.\n",
    "\n",
    "**3. The errors are normally distributed. (That is, smaller errors are more probable than larger errors, according to the familiar bell curve.)**\n",
    "\n",
    "**How can I check for this?**\n",
    "- Check the Omnibus value (see below).\n",
    "- Check the Jarque-Bera value (see below).\n",
    "- Build a QQ-Plot.\n",
    "\n",
    "**What can I do if it looks like I'm violating this assumption?**\n",
    "- Consider log-scaling your data.\n",
    "\n",
    "**4. The errors are homoskedastic. (That is, the errors have the same variance. The Greek word $\\sigma\\kappa\\epsilon\\delta\\acute{\\alpha}\\nu\\nu\\upsilon\\mu\\iota$ means \"to scatter\".)**\n",
    "\n",
    "**How can I check for this?**\n",
    "- Check the Durbin-Watson score (see below).\n",
    "- Conduct a Goldfeld-Quandt test.\n",
    "- Build an error plot, i.e. a plot of errors for a particular predictor (vs. the values of that predictor).\n",
    "\n",
    "**What can I do if it looks like I'm violating this assumption?**\n",
    "- Consider dropping extreme values.\n",
    "- Consider log-scaling your target.\n",
    "- Consider a different type of model!\n",
    "\n",
    "There is no general requirement that the predictor and the target *themselves* be normally distributed. However: Linear regression can work better if the predictor and target are normally distributed. Log-scaling can be a good tool to make data more normal.\n",
    "\n",
    "Suppose e.g. a kde plot of my predictor $X$ looks like this:\n",
    "\n",
    "![original](images/skewplot.png)\n",
    "\n",
    "In that case the kde plot of $log(X)$ looks like this:\n",
    "\n",
    "![log](images/logplot.png)\n",
    "\n",
    "[Here](https://www.statisticssolutions.com/assumptions-of-linear-regression/) is a helpful resource on the assumptions of linear regression.\n",
    "\n",
    "Experiment: [Playing with regression line](https://www.desmos.com/calculator/jwquvmikhr) <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations: [Anscombe's Quartet](https://www.desmos.com/calculator/paknt6oneh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = sns.load_dataset('anscombe')\n",
    "sns.scatterplot(data=ans, x='x', y='y', hue='dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using best_line\n",
    "\n",
    "Let's take a look at the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_line(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-fit line exists no matter what my data look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rand = stats.uniform.rvs(size=100)\n",
    "Y_rand = stats.uniform.rvs(size=100)\n",
    "\n",
    "best_line(X_rand, Y_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting X so that the intercept term of the best-fit line will be 0\n",
    "X = np.array([1.5, 3.5, 5.5])\n",
    "Y = np.array([2, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X.reshape(-1, 1), Y)\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(m):\n",
    "    line = m*X\n",
    "    err = sum(x**2 for x in [line - model.predict(X.reshape(-1, 1))])\n",
    "    return sum(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ms = np.linspace(0, 5, 100)\n",
    "ys = [sse(m) for m in ms]\n",
    "\n",
    "ax.plot(ms, ys);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going 3d to plot error as a function of both m and b\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def new_sse(m, x, b, y):\n",
    "    \"\"\"\n",
    "    This function returns the sum of squared errors for\n",
    "    a target y and a linear estimate mx + b.\n",
    "    \"\"\"\n",
    "    return len(x) * metrics.mean_squared_error(y, m*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going back to our original example\n",
    "X_sample = np.array([1, 3, 5])\n",
    "Y_sample = np.array([2, 9, 10])\n",
    "\n",
    "# This should be our minimum error\n",
    "new_sse(2, X_sample, 1, Y_sample)\n",
    "\n",
    "ms = np.linspace(-3, 7, 100)\n",
    "bs = np.linspace(-5, 5, 100)\n",
    "\n",
    "X_grid, Y_grid = np.meshgrid(ms, bs)\n",
    "\n",
    "Z = np.array([[new_sse(m, X_sample, b, Y_sample) for m in ms] for b in bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_errs = {}\n",
    "for m in ms:\n",
    "    m_errs[m] = new_sse(m, X_sample, 1, Y_sample)\n",
    "print(min(m_errs.values()))\n",
    "for k in m_errs:\n",
    "    if m_errs[k] == min(m_errs.values()):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_errs = {}\n",
    "for b in bs:\n",
    "    b_errs[b] = new_sse(2, X_sample, b, Y_sample)\n",
    "print(min(b_errs.values()))\n",
    "for k in b_errs:\n",
    "    if b_errs[k] == min(b_errs.values()):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X_grid, Y_grid, Z)\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('y-intercept')\n",
    "ax.set_zlabel('sum of squared errors')\n",
    "plt.title('Error as a function of slope and y-intercept');\n",
    "plt.savefig('images/surfacePlotSSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X_grid, Y_grid, Z, 200)\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('y-intercept')\n",
    "ax.set_zlabel('sum of squared errors')\n",
    "plt.title('Error as a function of slope and y-intercept');\n",
    "plt.savefig('images/contourPlotSSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
